---
title: "Practical Machine Learning Course Project"
author: "Wendy Larsen"
date: "March 26, 2019"
output:
  html_document: default
  pdf_document: default
---

This report is produced and tested on RStudio Version 1.1.463.

### Background: 
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways:
* Class A - exactly according to the specification
* Class B - throwing the elbows to the front
* Class C - lifting the dumbbell only halfway
* Class D - lowering the dumbbell only halfway
* Class E - throwing the hips to the front

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. Researchers made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

### Data: 
The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

### Objective
The goal of this project is to predict the manner in which subjects did the exercise. The model will use the other variables to predict with. This report will describe:
* how the model is built
* use of cross validation
* an estimate of expected out of sample error

### Cross Validation
Cross-validation will be performed by subsampling the training data set -  80% of the original Training data will be used for subTraining and 20% for subTesting. The models will be fitted on the subTraining data set, and tested on the subTesting data. Once the model is chosen, it will be tested on the original testing data set

### Data Preparation
Step 1 - load the appropriate packages: caret, randomForest, rpart, rpart.plot
```{r include=FALSE}
options(repos=c(CRAN="USA(CA1)[https]"))
install.packages("caret")
install.packages("randomForest")
install.packages("rpart")
install.packages("rpart.plot")
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
```


Step 2 - set seed for reproducibility
```{r}
set.seed(1234)
```

Step 3 - download the data, load it into R, turn missing values into NA, and prepare it for modelling purposes
```{r}
trainingset <- read.csv("C:/Users/wshla/Documents/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))

testingset <- read.csv("C:/Users/wshla/Documents/pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))
```

Check dimensions
```{r}
dim(trainingset)
dim(testingset)
```

Remove columns with missing values
```{r}
trainingset<-trainingset[,colSums(is.na(trainingset)) == 0]
testingset <-testingset[,colSums(is.na(testingset)) == 0]
```

Remove unecessary columns
```{r}
trainingset   <-trainingset[,-c(1:7)]
testingset <-testingset[,-c(1:7)]
```

Look at cleaner dataset
```{r}
dim(trainingset)
dim(testingset)
```

### Partition the training data set to allow for cross validation
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
subsamples <- createDataPartition(y=trainingset$classe, p=0.8, list=FALSE)
subTraining <- trainingset[subsamples, ] 
subTesting <- trainingset[-subsamples, ]
dim(subTraining)
dim(subTesting)
head(subTraining)
head(subTesting)
```

Plot the data
```{r}
plot(subTraining$classe, col="blue", main="Bar Plot of levels of the variable classe within the subTraining data set", xlab="classe levels", ylab="Frequency")
```

Above graph displays frequency by classe with Level A being most frequent occurrences and Level D being lowest.

### Predictions
Model 1 - Using Decision Tree
```{r}
model1 <- rpart(classe ~ ., data=subTraining, method="class")

prediction1 <- predict(model1, subTesting, type = "class")

rpart.plot(model1, main="Classification Tree", extra=102, under=TRUE, faclen=0)
```

Test Results
```{r}
confusionMatrix(prediction1, subTesting$classe)
```

Model 2 - Using Random Forest
```{r}
model2 <- randomForest(classe ~. , data=subTraining, method="class")

prediction2 <- predict(model2, subTesting, type = "class")
```

Test Results
```{r}
confusionMatrix(prediction2, subTesting$classe)
```

### Preferred Model
The Random Forest algoritm performed better at 99.8% than Decision Tree at 75.17% accuracy and is therefore chosen.

### Apply to Test Set
Predict the outcome levels on the original testing data set using the Random Forest algorithm
```{r}
predictfinal <- predict(model2, testingset, type="class")
predictfinal
```

Thank you for reading my report.



